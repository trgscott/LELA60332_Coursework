{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trgscott/LELA60332_Coursework/blob/main/CL2_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Computational Linguistics 2 - NER Assignment**"
      ],
      "metadata": {
        "id": "2eIS3jsHg_wG"
      },
      "id": "2eIS3jsHg_wG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install libraries / prerequisites**\n",
        "\n",
        "A GPU runtime is also required."
      ],
      "metadata": {
        "id": "J4OQJNxVhUOO"
      },
      "id": "J4OQJNxVhUOO"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "DvXjdvYmaKNE"
      },
      "id": "DvXjdvYmaKNE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1853f72f-a465-4120-8a36-65916b65bdb6",
      "metadata": {
        "tags": [],
        "id": "1853f72f-a465-4120-8a36-65916b65bdb6"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "from urllib import request\n",
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "from random import shuffle\n",
        "from math import ceil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_scheduler, BitsAndBytesConfig\n",
        "import datasets\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from glob import glob\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the devices**"
      ],
      "metadata": {
        "id": "HUZ-5I4hgldR"
      },
      "id": "HUZ-5I4hgldR"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_device = \"cuda\"  # set to 0 when on MCR system, \"cuda\" when using Colab\n",
        "clf_head_device = \"cuda\" # set to 0 when on MCR system, \"cuda\" when using Colab\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu' # set to 0 when on MCR system, \"cuda\" when using Colab"
      ],
      "metadata": {
        "id": "qem8OflZgTdI"
      },
      "id": "qem8OflZgTdI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fix random seeds for reproducibility**"
      ],
      "metadata": {
        "id": "nCeg6OgAWz8b"
      },
      "id": "nCeg6OgAWz8b"
    },
    {
      "cell_type": "code",
      "source": [
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "UmyYtkM8XBR1"
      },
      "id": "UmyYtkM8XBR1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code provided with assignment instructions to download the data**"
      ],
      "metadata": {
        "id": "8_aglKz1hGyR"
      },
      "id": "8_aglKz1hGyR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e8fe76-d9ba-4e31-abbf-df797e9f6ff9",
      "metadata": {
        "tags": [],
        "id": "35e8fe76-d9ba-4e31-abbf-df797e9f6ff9"
      },
      "outputs": [],
      "source": [
        "def parse_conllu_using_pandas(block):\n",
        "    records = []\n",
        "    for line in block.splitlines():\n",
        "        if not line.startswith('#'):\n",
        "            records.append(line.strip().split('\\t'))\n",
        "    return pd.DataFrame.from_records(\n",
        "        records,\n",
        "        columns=['ID', 'FORM', 'TAG', 'Misc1', 'Misc2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35cb93ef-bdd8-4b75-bfb0-78e0b55efa55",
      "metadata": {
        "tags": [],
        "id": "35cb93ef-bdd8-4b75-bfb0-78e0b55efa55"
      },
      "outputs": [],
      "source": [
        "def tokens_to_labels(df):\n",
        "    return (\n",
        "        df.FORM.tolist(),\n",
        "        df.TAG.tolist()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da10e1c-5b6d-4317-8444-2a9ba7643246",
      "metadata": {
        "tags": [],
        "id": "1da10e1c-5b6d-4317-8444-2a9ba7643246"
      },
      "outputs": [],
      "source": [
        "PREFIX = \"https://raw.githubusercontent.com/UniversalNER/\"\n",
        "DATA_URLS = {\n",
        "    \"en_ewt\": {\n",
        "        \"train\": \"UNER_English-EWT/master/en_ewt-ud-train.iob2\",\n",
        "        \"dev\": \"UNER_English-EWT/master/en_ewt-ud-dev.iob2\",\n",
        "        \"test\": \"UNER_English-EWT/master/en_ewt-ud-test.iob2\"\n",
        "    },\n",
        "    \"en_pud\": {\n",
        "        \"test\": \"UNER_English-PUD/master/en_pud-ud-test.iob2\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb40dbdf-a048-4691-8f95-63d4926d2cca",
      "metadata": {
        "tags": [],
        "id": "eb40dbdf-a048-4691-8f95-63d4926d2cca"
      },
      "outputs": [],
      "source": [
        "# en_ewt is the main train-dev-test split\n",
        "# en_pud is the OOD test set\n",
        "data_dict = defaultdict(dict)\n",
        "for corpus, split_dict in DATA_URLS.items():\n",
        "    for split, url_suffix in split_dict.items():\n",
        "        url = PREFIX + url_suffix\n",
        "        with request.urlopen(url) as response:\n",
        "            txt = response.read().decode('utf-8')\n",
        "            data_frames = map(parse_conllu_using_pandas,\n",
        "                              txt.split('\\n\\n'))\n",
        "            token_label_alignments = list(map(tokens_to_labels,\n",
        "                                              data_frames))\n",
        "            data_dict[corpus][split] = token_label_alignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba859ac8-66e1-4c4e-9647-b47f90677b54",
      "metadata": {
        "tags": [],
        "id": "ba859ac8-66e1-4c4e-9647-b47f90677b54"
      },
      "outputs": [],
      "source": [
        "# Saving the data so that you don't have to redownload it each time.\n",
        "with open('ner_data_dict.json', 'w', encoding='utf-8') as out:\n",
        "    json.dump(data_dict, out, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f7e422-4337-40b3-9b09-fdf4d5a91afd",
      "metadata": {
        "tags": [],
        "id": "71f7e422-4337-40b3-9b09-fdf4d5a91afd"
      },
      "outputs": [],
      "source": [
        "# Each subset of each corpus is a list of tuples where each tuple\n",
        "# is a list of tokens with a corresponding list of labels.\n",
        "\n",
        "# Train on data_dict['en_ewt']['train']; validate on data_dict['en_ewt']['dev']\n",
        "# and test on data_dict['en_ewt']['test'] and data_dict['en_pud']['test']\n",
        "data_dict['en_ewt']['train'][0], data_dict['en_pud']['test'][6]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preparation**"
      ],
      "metadata": {
        "id": "jMEosALccXOp"
      },
      "id": "jMEosALccXOp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pair the tokens with the labels in sentences, with seven labels**"
      ],
      "metadata": {
        "id": "MKopMSsH4sUI"
      },
      "id": "MKopMSsH4sUI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to pair the tokens with the labels within sentences\n",
        "def pair_data(data):\n",
        "  paired_data = []\n",
        "  for tokens, labels in data:\n",
        "    paired_example = []\n",
        "    for i in range(len(tokens)):\n",
        "      paired_example.append([tokens[i], labels[i]])\n",
        "    paired_data.append(paired_example)\n",
        "\n",
        "  labels = [label for sentence in paired_data for _, label in sentence]\n",
        "  label_counts = Counter(labels)\n",
        "  print(label_counts)\n",
        "\n",
        "  return paired_data"
      ],
      "metadata": {
        "id": "s8hw9UZ0gg5U"
      },
      "id": "s8hw9UZ0gg5U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pair the data and remove the final empty sentence from each dataset\n",
        "paired_data_train = pair_data(data_dict['en_ewt']['train'][0:12543])\n",
        "paired_data_dev = pair_data(data_dict['en_ewt']['dev'][0:2001])\n",
        "paired_data_test = pair_data(data_dict['en_ewt']['test'][0:2077])\n",
        "paired_data_OoD = pair_data(data_dict['en_pud']['test'][0:1000])"
      ],
      "metadata": {
        "id": "KNvFu6r7hLom"
      },
      "id": "KNvFu6r7hLom",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check lengths now match the Universal NER paper / original source of data (12543, 2001, 2077, 1000)\n",
        "print(len(paired_data_train))\n",
        "print(len(paired_data_dev))\n",
        "print(len(paired_data_test))\n",
        "print(len(paired_data_OoD))"
      ],
      "metadata": {
        "id": "644LZJSmDejq"
      },
      "id": "644LZJSmDejq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffle by sentence\n",
        "random.shuffle(paired_data_train)\n",
        "random.shuffle(paired_data_dev)\n",
        "random.shuffle(paired_data_test)\n",
        "random.shuffle(paired_data_OoD)"
      ],
      "metadata": {
        "id": "B437Y9oSSpfN"
      },
      "id": "B437Y9oSSpfN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confirm format / random seed matching - should read 'It would have been more than one could bear!'\n",
        "paired_data_train[1:2]"
      ],
      "metadata": {
        "id": "oLoVGVVv6hAo"
      },
      "id": "oLoVGVVv6hAo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create new versions of the shuffled paired data with three labels - 'B', 'I', 'O'**"
      ],
      "metadata": {
        "id": "yrvk0BDmc91Q"
      },
      "id": "yrvk0BDmc91Q"
    },
    {
      "cell_type": "code",
      "source": [
        "### Method to subsitute one label ###\n",
        "def substitute_labels(data, old_label, new_label):\n",
        "  new_data = []\n",
        "  for sentence in data:\n",
        "    new_example = []\n",
        "    for token, label in sentence:\n",
        "      if label == old_label:\n",
        "        new_example.append([token, new_label])\n",
        "      else:\n",
        "        new_example.append([token, label])\n",
        "    new_data.append(new_example)\n",
        "\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "guNkxhXbg2Yj"
      },
      "id": "guNkxhXbg2Yj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method to substitute all labels\n",
        "def substitute_all_labels(data):\n",
        "  new_one = substitute_labels(data, \"B-LOC\", \"B\")\n",
        "  new_two = substitute_labels(new_one, \"B-ORG\", \"B\")\n",
        "  new_three = substitute_labels(new_two, \"B-PER\", \"B\")\n",
        "  new_four = substitute_labels(new_three, \"I-LOC\", \"I\")\n",
        "  new_five = substitute_labels(new_four, \"I-ORG\", \"I\")\n",
        "  new_six = substitute_labels(new_five, \"I-PER\", \"I\")\n",
        "  return new_six"
      ],
      "metadata": {
        "id": "97HJWXl0pEkk"
      },
      "id": "97HJWXl0pEkk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitute the labels and create the new data versions\n",
        "three_data_train = substitute_all_labels(paired_data_train)\n",
        "three_data_dev = substitute_all_labels(paired_data_dev)\n",
        "three_data_test = substitute_all_labels(paired_data_test)\n",
        "three_data_OoD = substitute_all_labels(paired_data_OoD)"
      ],
      "metadata": {
        "id": "0ObAA9zKpfPL"
      },
      "id": "0ObAA9zKpfPL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the new labels\n",
        "three_labels = [label for sentence in three_data_train for _, label in sentence]\n",
        "three_label_counts = Counter(three_labels)\n",
        "print(f'Training data label counts: {three_label_counts}')"
      ],
      "metadata": {
        "id": "HJ_gRQA9rpE8"
      },
      "id": "HJ_gRQA9rpE8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create relevant labels and number of classes**"
      ],
      "metadata": {
        "id": "RMslFDSVHd4_"
      },
      "id": "RMslFDSVHd4_"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_labels_and_classes(data):\n",
        "  labels = set()\n",
        "  for ex in data:\n",
        "    labels.update([el[1] for el in ex])\n",
        "  n_classes = len(labels)\n",
        "  return sorted(labels), n_classes"
      ],
      "metadata": {
        "id": "qvzyIbkEMUs8"
      },
      "id": "qvzyIbkEMUs8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seven = create_labels_and_classes(paired_data_train)\n",
        "three = create_labels_and_classes(three_data_train)\n",
        "print(seven)\n",
        "print(three)"
      ],
      "metadata": {
        "id": "SOSjDtTmMnxY"
      },
      "id": "SOSjDtTmMnxY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set the task - three labels or seven labels - reset as needed**"
      ],
      "metadata": {
        "id": "kgm3K0PmqpbV"
      },
      "id": "kgm3K0PmqpbV"
    },
    {
      "cell_type": "code",
      "source": [
        "#Change THIS_MANY_LABELS to three or seven depending on the task\n",
        "\n",
        "\n",
        "\n",
        "THIS_MANY_LABELS = seven\n",
        "\n",
        "\n",
        "# Do not change the below\n",
        "labels = three[0] if THIS_MANY_LABELS == three else seven[0]\n",
        "n_classes = three[1] if THIS_MANY_LABELS == three else seven[1]"
      ],
      "metadata": {
        "id": "ro-OR7IEZI5n"
      },
      "id": "ro-OR7IEZI5n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder only - BERT**"
      ],
      "metadata": {
        "id": "7soE8yDF-IJE"
      },
      "id": "7soE8yDF-IJE"
    },
    {
      "cell_type": "code",
      "source": [
        "#Using cased version of BERT as likely better for NER / proper nouns\n",
        "model_tag = 'google-bert/bert-base-cased'\n",
        "tokeniser = AutoTokenizer.from_pretrained(model_tag)\n",
        "encoder = AutoModel.from_pretrained(model_tag).to(encoder_device)"
      ],
      "metadata": {
        "id": "yLYTc02CVRup"
      },
      "id": "yLYTc02CVRup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification head, with dropout and hidden layer,\n",
        "# in similar vein to encoder structure (Gelu and 0.1 dropout)\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, model_dim=768, n_classes=n_classes): # classes defined above\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(model_dim, model_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear2 = nn.Linear(model_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.linear2(x)"
      ],
      "metadata": {
        "id": "DY2PjekNcXuT"
      },
      "id": "DY2PjekNcXuT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_head = ClassificationHead(n_classes=n_classes) # classes defined above\n",
        "clf_head.to(clf_head_device);"
      ],
      "metadata": {
        "id": "soNLHNTlYTFz"
      },
      "id": "soNLHNTlYTFz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with freezing first few layers of the encoder:\n",
        "\n",
        "#for name, param in encoder.named_parameters():\n",
        "#    if name.startswith(\"bert.encoder.layer.0\") or name.startswith(\"bert.encoder.layer.1\"):\n",
        "#        param.requires_grad = False\n",
        "\n",
        "#optimizer_parameters = [param for name, param in encoder.named_parameters()\n",
        "#    if not (name.startswith(\"bert.encoder.layer.0\") or name.startswith(\"bert.encoder.layer.1\"))\n",
        "#] + list(clf_head.parameters())\n",
        "\n",
        "#https://datascientistsdiary.com/fine-tuning-bert-a-practical-guide/#:~:text=Schedulers%20fix%20this.%20The%20best%20scheduler%20for,for%2010%25%20of%20training%20lr_scheduler%20=%20get_scheduler"
      ],
      "metadata": {
        "id": "0HocSgBVE-5e"
      },
      "id": "0HocSgBVE-5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sentence(sentence, label_to_i, tokeniser, encoder, clf_head,\n",
        "                     encoder_device, clf_head_device):\n",
        "    gold_labels = torch.tensor(\n",
        "        [label_to_i[label] for _, label in sentence]).to(clf_head_device)\n",
        "    words = [word for word, _ in sentence]\n",
        "    tokenisation = tokeniser(words, is_split_into_words=True,\n",
        "                             return_tensors='pt')\n",
        "    inputs = {k: v.to(encoder_device) for k, v in tokenisation.items()}\n",
        "\n",
        "    # Don't need the embeddings of the CLS or SEP tokens\n",
        "    outputs = encoder(**inputs).last_hidden_state[0, 1:-1, :]\n",
        "\n",
        "    # Take the first subword. The logic is that we will fine-tune the\n",
        "    # encoder as well, and we hope that it will learn to channel all the\n",
        "    # necessary information into first subwords.\n",
        "    # Note that word_ids are found only in the original tokeniser output,\n",
        "    # in the dictionary with tensors copied to the GPU.\n",
        "    # We ignore the CLS and the SEP tokens\n",
        "    word_ids = tokenisation.word_ids()[1:-1]\n",
        "    processed_words = set()\n",
        "    first_subword_embeddings = []\n",
        "    # Indices of subwords in outputs are aligned with word_ids, so we can use\n",
        "    # the same indices in both arrays.\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        if word_id not in processed_words:\n",
        "            first_subword_embeddings.append(outputs[i])\n",
        "            processed_words.add(word_id)\n",
        "\n",
        "    # Check that we aligned words and labels correctly.\n",
        "    assert len(first_subword_embeddings) == gold_labels.size(0)\n",
        "\n",
        "    # Combine subword embeddings into a tensor and copy to the device\n",
        "    # where the classifier head resides.\n",
        "    clf_head_inputs = torch.vstack(\n",
        "        first_subword_embeddings).to(clf_head_device)\n",
        "\n",
        "    # Return the logits and gold labels for subsequent processing\n",
        "    return clf_head(clf_head_inputs), gold_labels"
      ],
      "metadata": {
        "id": "gRXq7LW-WGun"
      },
      "id": "gRXq7LW-WGun",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(data, label_to_i, tokeniser, encoder, clf_head,\n",
        "                encoder_device, clf_head_device, loss_fn, optimiser):\n",
        "    encoder.train()\n",
        "    epoch_losses = torch.empty(len(data))\n",
        "    for step_n, sentence in tqdm(\n",
        "        enumerate(data),\n",
        "        total=len(data),\n",
        "        desc='Train',\n",
        "        leave=False\n",
        "    ):\n",
        "        optimiser.zero_grad()\n",
        "        logits, gold_labels = process_sentence(\n",
        "            sentence, label_to_i, tokeniser,\n",
        "            encoder, clf_head, encoder_device,\n",
        "            clf_head_device)\n",
        "        loss = loss_fn(logits, gold_labels)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        epoch_losses[step_n] = loss.item()\n",
        "    return epoch_losses.mean().item()"
      ],
      "metadata": {
        "id": "GsT9lYipYM14"
      },
      "id": "GsT9lYipYM14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_epoch(data, label_to_i, tokeniser, encoder, clf_head,\n",
        "                   encoder_device, clf_head_device):\n",
        "    encoder.eval()\n",
        "    epoch_accuracies = torch.empty(len(data))\n",
        "    all_predictions = [] # for the f1 scoring\n",
        "    all_labels = [] # for the f1 scoring\n",
        "    for step_n, sentence in tqdm(\n",
        "        enumerate(data),\n",
        "        total=len(data),\n",
        "        desc='Eval',\n",
        "        leave=False\n",
        "    ):\n",
        "        with torch.no_grad():\n",
        "            logits, gold_labels = process_sentence(\n",
        "                sentence, label_to_i, tokeniser,\n",
        "                encoder, clf_head, encoder_device,\n",
        "                clf_head_device)\n",
        "        predicted_labels = torch.argmax(logits, dim=-1)\n",
        "        epoch_accuracies[step_n] = (predicted_labels == gold_labels).sum().item() / len(sentence)\n",
        "        # collect the predictions and gold labels for the f1 scoring\n",
        "        all_predictions.extend(predicted_labels.cpu().numpy())\n",
        "        all_labels.extend(gold_labels.cpu().numpy())\n",
        "\n",
        "    f1_scores = {}\n",
        "    precision_scores = {}\n",
        "    recall_scores = {}\n",
        "    # calculate the scores for each class\n",
        "    for label_index in range(n_classes):\n",
        "      #f1 calculation\n",
        "      f1 = f1_score(np.array(all_labels) == label_index, np.array(all_predictions) == label_index, average='binary')\n",
        "      f1_scores[i_to_label[label_index]] = f1\n",
        "      #precision calculation\n",
        "      precision = precision_score(np.array(all_labels) == label_index, np.array(all_predictions) == label_index, average='binary')\n",
        "      precision_scores[i_to_label[label_index]] = precision\n",
        "      #recall calculation\n",
        "      recall = recall_score(np.array(all_labels) == label_index, np.array(all_predictions) == label_index, average='binary')\n",
        "      recall_scores[i_to_label[label_index]] = recall\n",
        "\n",
        "    # calculate the f1 macro average\n",
        "    macro_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "    # calculate the f1 micro average\n",
        "    micro_f1 = f1_score(all_labels, all_predictions, average='micro')\n",
        "\n",
        "    return epoch_accuracies.mean().item(), f1_scores, macro_f1, predicted_labels, micro_f1, precision_scores, recall_scores\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
      ],
      "metadata": {
        "id": "qoMWDg6BCE7o"
      },
      "id": "qoMWDg6BCE7o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method to compute span accuracy - correct sentences / total sentences\n",
        "def span_accuracy_encoder(data):\n",
        "  encoder.eval()\n",
        "  correct_sentences = 0\n",
        "  total_sentences = len(data)\n",
        "  for sentence in data:\n",
        "      with torch.no_grad():\n",
        "          logits, gold_labels = process_sentence(\n",
        "              sentence, label_to_i, tokeniser,\n",
        "              encoder, clf_head, encoder_device,\n",
        "              clf_head_device\n",
        "          )\n",
        "      predicted_labels = torch.argmax(logits, dim=-1)\n",
        "      if torch.all(predicted_labels == gold_labels):\n",
        "          correct_sentences += 1\n",
        "  span_accuracy_encoder = correct_sentences / total_sentences\n",
        "  return span_accuracy_encoder"
      ],
      "metadata": {
        "id": "V_iz2bTiZ-AS"
      },
      "id": "V_iz2bTiZ-AS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING #\n",
        "\n",
        "#Ensure correct data is being used based on label choice made further above\n",
        "if THIS_MANY_LABELS == three:\n",
        "  training_data = three_data_train\n",
        "  dev_data = three_data_dev\n",
        "  test_data = three_data_test\n",
        "  OoD_data = three_data_OoD\n",
        "elif THIS_MANY_LABELS == seven:\n",
        "    training_data = paired_data_train\n",
        "    dev_data = paired_data_dev\n",
        "    test_data = paired_data_test\n",
        "    OoD_data = paired_data_OoD\n",
        "\n",
        "print(f'You are running this with {n_classes} labels, are you sure?')\n",
        "print(f'Your labels are {labels}')\n",
        "\n",
        "# The models expect class numbers, not strings\n",
        "# Code is here to make sure this changes depending on the number of labels chosen\n",
        "label_to_i = {\n",
        "    label: i\n",
        "    for i, label in enumerate(sorted(labels))\n",
        "}\n",
        "i_to_label = {\n",
        "    i: label\n",
        "    for label, i in label_to_i.items()\n",
        "}\n",
        "\n",
        "#Hyperparameters\n",
        "n_epochs = 8\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#change to (optimizer_parameters, lr=10**(-5)) if freezing layers\n",
        "optimiser = torch.optim.AdamW(list(encoder.parameters()) + list(clf_head.parameters()), lr=10**(-5))\n",
        "\n",
        "#Early stopping set up, based on improving macro_f1\n",
        "best_f1 = 0\n",
        "last_epoch_with_dev_improvement = 0\n",
        "n_epochs_without_improvement = 0\n",
        "early_stopping_threshold = 3\n",
        "\n",
        "#Training, validation and early stopping\n",
        "for epoch_n in tqdm(range(n_epochs)):\n",
        "\n",
        "    loss = train_epoch(training_data, label_to_i, tokeniser, encoder, clf_head, encoder_device, clf_head_device, loss_fn, optimiser)\n",
        "    print(f'Epoch {epoch_n+1} training loss: {loss:.2f}')\n",
        "\n",
        "    accuracy, _, macro_f1, _, micro_f1, _, _ = validate_epoch(dev_data, label_to_i, tokeniser, encoder, clf_head, encoder_device, clf_head_device)\n",
        "    print(f'Epoch {epoch_n+1} dev accuracy: {accuracy:.2f}')\n",
        "    print(f'Epoch {epoch_n+1} dev macro f1: {macro_f1:.2f}')\n",
        "    print(f'Epoch {epoch_n+1} dev micro f1: {micro_f1:.2f}')\n",
        "\n",
        "    if macro_f1 > best_f1:\n",
        "      best_f1 = macro_f1\n",
        "      last_epoch_with_dev_improvement = epoch_n\n",
        "      for path in glob('*.pt'):\n",
        "          os.remove(path)\n",
        "      torch.save(encoder.state_dict(), 'best_encoder.pt')\n",
        "      torch.save(clf_head.state_dict(), 'best_clf_head.pt')\n",
        "    else:\n",
        "      n_epochs_without_improvement = epoch_n - last_epoch_with_dev_improvement\n",
        "      if n_epochs_without_improvement == early_stopping_threshold:\n",
        "          print(f'{n_epochs_without_improvement} without improvement; early stopping.')\n",
        "          break"
      ],
      "metadata": {
        "id": "ILmFOZbqYUPR"
      },
      "id": "ILmFOZbqYUPR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the saved model as required\n",
        "# encoder_best = encoder()\n",
        "# encoder_best.load_state_dict(torch.load('best_encoder.pt', weights_only=True))"
      ],
      "metadata": {
        "id": "fG4UYK0E67Mw"
      },
      "id": "fG4UYK0E67Mw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING ON THE TEST SETS**"
      ],
      "metadata": {
        "id": "8zR8bh2UTGEc"
      },
      "id": "8zR8bh2UTGEc"
    },
    {
      "cell_type": "code",
      "source": [
        "#Test set\n",
        "_, f1_scores, macro_f1, _, micro_f1, precision_scores, recall_scores = validate_epoch(test_data, label_to_i, tokeniser, encoder, clf_head, encoder_device, clf_head_device)\n",
        "\n",
        "print(f'Epoch {epoch_n+1} test accuracy: {accuracy:.2f}')\n",
        "\n",
        "print(f'Epoch {epoch_n+1} test macro f1: {macro_f1:.2f}')\n",
        "\n",
        "print(\"F1\")\n",
        "for label, f1 in f1_scores.items():\n",
        "  print(f\"{label}: {f1}\")\n",
        "\n",
        "print(\"Precision\")\n",
        "for label, precision in precision_scores.items():\n",
        "  print(f\"{label}: {precision}\")\n",
        "\n",
        "print(\"Recall\")\n",
        "for label, recall in recall_scores.items():\n",
        "  print(f\"{label}: {recall}\")\n",
        "\n",
        "print(f\"Span accuracy on test set: {span_accuracy_encoder(test_data)}\")"
      ],
      "metadata": {
        "id": "w-6je5bzQrT9"
      },
      "id": "w-6je5bzQrT9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Out of Domain test set\n",
        "_, f1_scores, macro_f1, _, micro_f1, precision_scores, recall_scores = validate_epoch(OoD_data, label_to_i, tokeniser, encoder, clf_head, encoder_device, clf_head_device)\n",
        "\n",
        "print(f'Epoch {epoch_n+1} OoD accuracy: {accuracy:.2f}')\n",
        "\n",
        "print(f'Epoch {epoch_n+1} OoD macro f1: {macro_f1:.2f}')\n",
        "\n",
        "print(\"F1\")\n",
        "for label, f1 in f1_scores.items():\n",
        "  print(f\"{label}: {f1}\")\n",
        "\n",
        "print(\"Precision\")\n",
        "for label, precision in precision_scores.items():\n",
        "  print(f\"{label}: {precision}\")\n",
        "\n",
        "print(\"Recall\")\n",
        "for label, recall in recall_scores.items():\n",
        "  print(f\"{label}: {recall}\")\n",
        "\n",
        "print(f\"Span accuracy on OoD set: {span_accuracy_encoder(OoD_data)}\")"
      ],
      "metadata": {
        "id": "lmcj1gdNRYB0"
      },
      "id": "lmcj1gdNRYB0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT model configuration**"
      ],
      "metadata": {
        "id": "XA2hv1Y_sNSI"
      },
      "id": "XA2hv1Y_sNSI"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_config = encoder.config\n",
        "encoder_config"
      ],
      "metadata": {
        "id": "jppBk05cvoqX"
      },
      "id": "jppBk05cvoqX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder"
      ],
      "metadata": {
        "id": "D12yWBnzqncB"
      },
      "id": "D12yWBnzqncB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_head"
      ],
      "metadata": {
        "id": "d_X6Tv7KKLE1"
      },
      "id": "d_X6Tv7KKLE1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder-Decoder - T5-Small**"
      ],
      "metadata": {
        "id": "08_AY08saidV"
      },
      "id": "08_AY08saidV"
    },
    {
      "cell_type": "code",
      "source": [
        "model_tag = 'google-t5/t5-small'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_tag, cache_dir='./hf_cache').to(device)\n",
        "tokeniser = AutoTokenizer.from_pretrained(model_tag)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=10**(-4)) # T5 paper suggests 10**(-3) for fine-tune lr, but 10**(-4) appears to work better"
      ],
      "metadata": {
        "id": "YEF12adMUM1c"
      },
      "id": "YEF12adMUM1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(batch_inputs, batch_labels,\n",
        "                  tokeniser, model, device,\n",
        "                  optimiser, max_len=512):\n",
        "    optimiser.zero_grad()\n",
        "    tokenisation = tokeniser(\n",
        "        batch_inputs,\n",
        "        return_tensors='pt',\n",
        "        max_length=max_len,\n",
        "        padding='longest',\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids = tokenisation.input_ids.to(device)\n",
        "    attention_mask = tokenisation.attention_mask.to(device)\n",
        "    labels = tokeniser(\n",
        "        batch_labels,\n",
        "        return_tensors='pt',\n",
        "        max_length=max_len,\n",
        "        padding='longest',\n",
        "        truncation=True\n",
        "    ).input_ids.to(device)\n",
        "    # Stop the model from generating pad tokens\n",
        "    labels[labels == tokeniser.pad_token_id] = -100\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "    loss = model(**inputs).loss\n",
        "    #print(f'loss: {loss.item():.3f}') # checking if loss is reducing since it always seems to be zero - conclude it's to do with how averaged over sentences\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "Y1qS_MY8UQrL"
      },
      "id": "Y1qS_MY8UQrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each sentence to a batch of examples, one per target word,\n",
        "# and highlight the target word using tildes, e.g.:\n",
        "# '~ I ~ see a dog .' -> 'PRON'\n",
        "# 'I ~ see ~ a dog .' -> 'VERB'\n",
        "# 'I see ~ a ~ dog .' -> 'DET'\n",
        "# etc.\n",
        "# Batch size will now control how many words we will analyse at the\n",
        "# same time.\n",
        "\n",
        "def prepare_sentence(sentence_array):\n",
        "    words = []; labels = []\n",
        "    for word, label in sentence_array:\n",
        "        words.append(word); labels.append(label)\n",
        "    prepared_inputs = []\n",
        "    for i in range(len(words)):\n",
        "        tmp = words[:i] + ['~', words[i], '~'] + words[i+1:]\n",
        "        prepared_inputs.append(' '.join(tmp))\n",
        "    return prepared_inputs, labels"
      ],
      "metadata": {
        "id": "3p-kZfzYbjmF"
      },
      "id": "3p-kZfzYbjmF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_sentence(paired_data_train[1]) # confirm prepare_sentence works and matches random shuffle above - should read 'It would have been more than one could bear!'"
      ],
      "metadata": {
        "id": "gw1wkb4WWFjd"
      },
      "id": "gw1wkb4WWFjd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_inputs, batch_size,\n",
        "                tokeniser, model, device, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    n_steps = len(train_inputs)\n",
        "    epoch_losses = torch.zeros(n_steps)\n",
        "    for step_n in tqdm(range(n_steps), leave=False, desc='Train'):\n",
        "        prepared_inputs, labels = prepare_sentence(train_inputs[step_n])\n",
        "        # Split the sentence in batches if it is long enough\n",
        "        n_batches = ceil(len(prepared_inputs) / batch_size)\n",
        "        sentence_losses_accum = 0.0 # since sentences are repeated multiple times, need to get losses for each version of sentence and accumulate then average\n",
        "        for step_n in range(n_batches):\n",
        "            lo = step_n * batch_size\n",
        "            hi = lo + batch_size\n",
        "            batch_texts = prepared_inputs[lo:hi]\n",
        "            batch_labels = labels[lo:hi]\n",
        "            loss = process_batch(batch_texts, batch_labels,\n",
        "                                 tokeniser, model, device,\n",
        "                                 optimizer)\n",
        "            #print(f'{loss:.3f}', end=' ') # checking losses again\n",
        "            sentence_losses_accum += loss\n",
        "        epoch_losses[step_n] = sentence_losses_accum / n_batches\n",
        "    return epoch_losses.mean().item()"
      ],
      "metadata": {
        "id": "OmmCHSX_boPT"
      },
      "id": "OmmCHSX_boPT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class_prediction(prompt, tokeniser, model, device, max_len=512):\n",
        "    tokenisation = tokeniser(\n",
        "        prompt,\n",
        "        return_tensors='pt',\n",
        "        max_length=max_len,\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids = tokenisation.input_ids.to(device)\n",
        "    attention_mask = tokenisation.attention_mask.to(device)\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "    max_new_tokens=4).squeeze() # squeeze deletes batch dimension, max_new_tokens limited to 4 as only need one - just in case of random special token\n",
        "\n",
        "    # Take the first word\n",
        "    output_string = tokeniser.decode(\n",
        "        output,\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "    if not output_string:\n",
        "        # Empty output\n",
        "        return None\n",
        "    return output_string.split()[0]"
      ],
      "metadata": {
        "id": "u-xqLjzwbq7Z"
      },
      "id": "u-xqLjzwbq7Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_epoch(dev_inputs, tokeniser, model, device, max_len=512):\n",
        "    model.eval()\n",
        "    n_steps = len(dev_inputs)\n",
        "    epoch_hits = []\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    for step_n in tqdm(range(n_steps), leave=False, desc='Validate'):\n",
        "        prepared_inputs, labels = prepare_sentence(dev_inputs[step_n])\n",
        "        with torch.no_grad():\n",
        "            for input_sentence, gold_label in zip(prepared_inputs, labels):\n",
        "                predicted_label = get_class_prediction(\n",
        "                    input_sentence, tokeniser, model, device,\n",
        "                    max_len=max_len)\n",
        "                epoch_hits.append(int(predicted_label == gold_label))\n",
        "                all_predictions.append(predicted_label)\n",
        "                all_labels.append(gold_label)\n",
        "\n",
        "    labels_set = set(all_labels)\n",
        "    f1_scores = {}\n",
        "    for label in labels_set:\n",
        "        f1 = f1_score(np.array(all_labels) == label, np.array(all_predictions) == label, average=\"binary\")\n",
        "        f1_scores[label] = f1\n",
        "\n",
        "    macro_f1 = f1_score(all_labels, all_predictions, average=\"macro\")\n",
        "\n",
        "    return sum(epoch_hits) / len(epoch_hits), f1_scores, macro_f1"
      ],
      "metadata": {
        "id": "ROU77MpObtIi"
      },
      "id": "ROU77MpObtIi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method to calculate f1 scores for encoder-decoder\n",
        "def calculate_f1_scores(data, tokeniser, model, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    for sentence in tqdm(data, leave=False, desc='F1 scores'):\n",
        "        prepared_inputs, labels = prepare_sentence(sentence)\n",
        "        with torch.no_grad():\n",
        "            for input_sentence, gold_label in zip(prepared_inputs, labels):\n",
        "                predicted_label = get_class_prediction(input_sentence, tokeniser, model, device)\n",
        "                all_predictions.append(predicted_label)\n",
        "                all_labels.append(gold_label)\n",
        "\n",
        "    labels_set = set(all_labels)\n",
        "    f1_scores = {}\n",
        "    for label in labels_set:\n",
        "        f1 = f1_score(np.array(all_labels) == label, np.array(all_predictions) == label, average=\"binary\")\n",
        "        f1_scores[label] = f1\n",
        "\n",
        "    macro_f1 = f1_score(all_labels, all_predictions, average=\"macro\")\n",
        "    return f1_scores, macro_f1"
      ],
      "metadata": {
        "id": "yuIF9w9aXAU5"
      },
      "id": "yuIF9w9aXAU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method to calculate the span accuracy for encoder-decoder\n",
        "def span_accuracy(data):\n",
        "  model.eval()\n",
        "  correct_sentences = 0\n",
        "  total_sentences = len(data)\n",
        "  for sentence in data:\n",
        "      prepared_inputs, labels = prepare_sentence(sentence)\n",
        "      sentence_correct = True\n",
        "      for input_sentence, gold_label in zip(prepared_inputs, labels):\n",
        "          with torch.no_grad():\n",
        "              predicted_label = get_class_prediction(input_sentence, tokeniser, model, device)\n",
        "          if predicted_label != gold_label:\n",
        "              sentence_correct = False\n",
        "              break\n",
        "      if sentence_correct:\n",
        "          correct_sentences += 1\n",
        "\n",
        "  accuracy = correct_sentences / total_sentences\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "ZJaNb18ToqdG"
      },
      "id": "ZJaNb18ToqdG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING #\n",
        "\n",
        "#Ensure using correct data based on label choice further above\n",
        "if THIS_MANY_LABELS == three:\n",
        "  training_data = three_data_train\n",
        "  dev_data = three_data_dev\n",
        "  test_data = three_data_test\n",
        "  OoD_data = three_data_OoD\n",
        "elif THIS_MANY_LABELS == seven:\n",
        "    training_data = paired_data_train\n",
        "    dev_data = paired_data_dev\n",
        "    test_data = paired_data_test\n",
        "    OoD_data = paired_data_OoD\n",
        "\n",
        "print(f'You are running this with {n_classes} labels, are you sure?')\n",
        "print(f'Your labels are {labels}')\n",
        "\n",
        "n_epochs = 4\n",
        "batch_size = 256 # batch is just how many words so can be large\n",
        "\n",
        "#Early stopping set up, based on improving macro_f1\n",
        "best_f1 = 0\n",
        "last_epoch_with_dev_improvement = 0\n",
        "n_epochs_without_improvement = 0\n",
        "early_stopping_threshold = 2\n",
        "\n",
        "for epoch_n in tqdm(range(n_epochs)):\n",
        "    epoch_loss = train_epoch(training_data, batch_size, tokeniser, model, device, optim)\n",
        "    print(f'Epoch {epoch_n+1} loss:', round(epoch_loss, 2))\n",
        "\n",
        "    epoch_dev_accuracy, _, macro_f1 = validate_epoch(dev_data, tokeniser, model, device)\n",
        "    print(f'Epoch {epoch_n+1} dev accuracy: {epoch_dev_accuracy:.2f}')\n",
        "    print(f'Epoch {epoch_n+1} dev macro F1: {macro_f1:.2f}')\n",
        "\n",
        "    if macro_f1 > best_f1:\n",
        "      best_f1 = macro_f1\n",
        "      last_epoch_with_dev_improvement = epoch_n\n",
        "      print('Saving the model.')\n",
        "      for path in glob('*.pt'):\n",
        "          os.remove(path)\n",
        "      torch.save(model.state_dict(), 'best_t5_model.pt')\n",
        "    else:\n",
        "      n_epochs_without_improvement = epoch_n - last_epoch_with_dev_improvement\n",
        "      if n_epochs_without_improvement == early_stopping_threshold:\n",
        "          print(f'{n_epochs_without_improvement} without improvement; early stopping.')\n",
        "          break\n",
        "\n",
        "#loss is not a good indicator of how well the model is performing because of the way it is averaging over multiple sentences\n",
        "#dev accuaracy is not a good indicator either because if it's just predicting O it will get amazing accuracy\n",
        "#Macro f1 is a better measure to check performance because it solves for the above issues"
      ],
      "metadata": {
        "id": "iu-0a6G-bxdN"
      },
      "id": "iu-0a6G-bxdN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing on the test sets**"
      ],
      "metadata": {
        "id": "OQEI4u-bbDuK"
      },
      "id": "OQEI4u-bbDuK"
    },
    {
      "cell_type": "code",
      "source": [
        "#Test set\n",
        "print(f\"Span Accuracy on test set: {span_accuracy(test_data)}\")\n",
        "\n",
        "f1_scores, macro_f1 = calculate_f1_scores(test_data, tokeniser, model, device)\n",
        "\n",
        "print(\"F1 scores per label on test data:\")\n",
        "for label, score in f1_scores.items():\n",
        "    print(f\"{label}: {score}\")\n",
        "print(f\"Macro F1 score on test data: {macro_f1}\")"
      ],
      "metadata": {
        "id": "shEnNUpCXFMO"
      },
      "id": "shEnNUpCXFMO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Out of Domain test set\n",
        "print(f\"Span Accuracy on Out of Domain set: {span_accuracy(OoD_data)}\")\n",
        "\n",
        "f1_scores, macro_f1 = calculate_f1_scores(OoD_data, tokeniser, model, device)\n",
        "\n",
        "print(\"F1 scores per label on Out of Domain data:\")\n",
        "for label, score in f1_scores.items():\n",
        "    print(f\"{label}: {score}\")\n",
        "print(f\"Macro F1 score on Out of Domain data: {macro_f1}\")"
      ],
      "metadata": {
        "id": "XGVtbMA7bb2B"
      },
      "id": "XGVtbMA7bb2B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "K-4QOrRAQ3oO"
      },
      "id": "K-4QOrRAQ3oO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "fwvZHNuvReww"
      },
      "id": "fwvZHNuvReww",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}